{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:03.605331Z","iopub.execute_input":"2025-03-25T05:10:03.605511Z","iopub.status.idle":"2025-03-25T05:10:04.558575Z","shell.execute_reply.started":"2025-03-25T05:10:03.605492Z","shell.execute_reply":"2025-03-25T05:10:04.557645Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import nltk","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:32.886154Z","iopub.execute_input":"2025-03-25T05:10:32.886454Z","iopub.status.idle":"2025-03-25T05:10:33.945848Z","shell.execute_reply.started":"2025-03-25T05:10:32.886426Z","shell.execute_reply":"2025-03-25T05:10:33.944978Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import random","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:35.221043Z","iopub.execute_input":"2025-03-25T05:10:35.221496Z","iopub.status.idle":"2025-03-25T05:10:35.225391Z","shell.execute_reply.started":"2025-03-25T05:10:35.221467Z","shell.execute_reply":"2025-03-25T05:10:35.224434Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset\n\nds = load_dataset(\"stas/openwebtext-10k\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:37.377338Z","iopub.execute_input":"2025-03-25T05:10:37.377651Z","iopub.status.idle":"2025-03-25T05:10:40.988598Z","shell.execute_reply.started":"2025-03-25T05:10:37.377625Z","shell.execute_reply":"2025-03-25T05:10:40.987948Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/951 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64005686cce946369b1024f94b61997e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"openwebtext-10k.py:   0%|          | 0.00/3.08k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e9430387e8a49b09c95dbc5f96b5ece"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0000.parquet:   0%|          | 0.00/30.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e770c3947d34c1ebcd2a16094a5a86f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5abda69db04475187ae5fa21f4d8cf8"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"ds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T11:58:00.340609Z","iopub.execute_input":"2025-03-24T11:58:00.341302Z","iopub.status.idle":"2025-03-24T11:58:00.346997Z","shell.execute_reply.started":"2025-03-24T11:58:00.341259Z","shell.execute_reply":"2025-03-24T11:58:00.346123Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset=ds['train']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:47.386328Z","iopub.execute_input":"2025-03-25T05:10:47.386871Z","iopub.status.idle":"2025-03-25T05:10:47.390561Z","shell.execute_reply.started":"2025-03-25T05:10:47.386839Z","shell.execute_reply":"2025-03-25T05:10:47.389707Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"dataset[1]['text']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T12:03:04.511871Z","iopub.execute_input":"2025-03-24T12:03:04.512214Z","iopub.status.idle":"2025-03-24T12:03:04.517375Z","shell.execute_reply.started":"2025-03-24T12:03:04.512173Z","shell.execute_reply":"2025-03-24T12:03:04.516559Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def split_to_sentences(text):\n    sentences=[sentence for sentence in text.split('\\n')]\n    return sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:50.647927Z","iopub.execute_input":"2025-03-25T05:10:50.648230Z","iopub.status.idle":"2025-03-25T05:10:50.652125Z","shell.execute_reply.started":"2025-03-25T05:10:50.648204Z","shell.execute_reply":"2025-03-25T05:10:50.651204Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"sample_sentences_list = split_to_sentences(dataset[0]['text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:10:52.854931Z","iopub.execute_input":"2025-03-25T05:10:52.855219Z","iopub.status.idle":"2025-03-25T05:10:52.859246Z","shell.execute_reply.started":"2025-03-25T05:10:52.855197Z","shell.execute_reply":"2025-03-25T05:10:52.858364Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"sample_sentences_list=[s for s in sample_sentences_list if s.strip()]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:01.642602Z","iopub.execute_input":"2025-03-25T05:11:01.642957Z","iopub.status.idle":"2025-03-25T05:11:01.646809Z","shell.execute_reply.started":"2025-03-25T05:11:01.642927Z","shell.execute_reply":"2025-03-25T05:11:01.645830Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"print(\"top 5 sentences:\\n\")\n\nfor i in range(5):\n    print(sample_sentences_list[i])\n    print('*'*10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:03.773871Z","iopub.execute_input":"2025-03-25T05:11:03.774179Z","iopub.status.idle":"2025-03-25T05:11:03.780896Z","shell.execute_reply.started":"2025-03-25T05:11:03.774148Z","shell.execute_reply":"2025-03-25T05:11:03.779998Z"}},"outputs":[{"name":"stdout","text":"top 5 sentences:\n\nA magazine supplement with an image of Adolf Hitler and the title 'The Unreadable Book' is pictured in Berlin. No law bans “Mein Kampf” in Germany, but the government of Bavaria, holds the copyright and guards it ferociously. (Thomas Peter/REUTERS)\n**********\nThe city that was the center of Adolf Hitler’s empire is littered with reminders of the Nazi past, from the bullet holes that pit the fronts of many buildings to the hulking Luftwaffe headquarters that now house the Finance Ministry.\n**********\nWhat it doesn’t have, nor has it since 1945, are copies of Hitler’s autobiography and political manifesto, “Mein Kampf,” in its bookstores. The latest attempt to publish excerpts fizzled this week after the Bavarian government challenged it in court, although an expurgated copy appeared at newspaper kiosks around the country.\n**********\nBut in Germany — where keeping a tight lid on Hitler’s writings has become a rich tradition in itself — attitudes toward his book are slowly changing, and fewer people are objecting to its becoming more widely available.\n**********\nNo law bans “Mein Kampf” in Germany, but the government of Bavaria, where Hitler officially resided at the time of his 1945 suicide, holds the copyright and guards it ferociously. German-language copies that were printed before 1945 are legal, although they command a premium price, and the book is available in translation elsewhere in the world.\n**********\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"def tokenize_sentences(sentences_list):\n    tokenized_s_list=[]\n    for sentence in sentences_list:\n        sentence=sentence.lower()\n        tokenized_s=nltk.tokenize.word_tokenize(sentence)\n        tokenized_s_list.append(tokenized_s)\n\n    return tokenized_s_list\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:28.319758Z","iopub.execute_input":"2025-03-25T05:11:28.320208Z","iopub.status.idle":"2025-03-25T05:11:28.325253Z","shell.execute_reply.started":"2025-03-25T05:11:28.320171Z","shell.execute_reply":"2025-03-25T05:11:28.324266Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"sample_tokenized_sentences= tokenize_sentences(sample_sentences_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:30.707885Z","iopub.execute_input":"2025-03-25T05:11:30.708215Z","iopub.status.idle":"2025-03-25T05:11:30.729562Z","shell.execute_reply.started":"2025-03-25T05:11:30.708186Z","shell.execute_reply":"2025-03-25T05:11:30.728650Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"for i in range(5):\n    print(sample_tokenized_sentences[i])\n    print('*'*10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:35.134443Z","iopub.execute_input":"2025-03-25T05:11:35.134773Z","iopub.status.idle":"2025-03-25T05:11:35.140901Z","shell.execute_reply.started":"2025-03-25T05:11:35.134717Z","shell.execute_reply":"2025-03-25T05:11:35.139955Z"}},"outputs":[{"name":"stdout","text":"['a', 'magazine', 'supplement', 'with', 'an', 'image', 'of', 'adolf', 'hitler', 'and', 'the', 'title', \"'the\", 'unreadable', 'book', \"'\", 'is', 'pictured', 'in', 'berlin', '.', 'no', 'law', 'bans', '“', 'mein', 'kampf', '”', 'in', 'germany', ',', 'but', 'the', 'government', 'of', 'bavaria', ',', 'holds', 'the', 'copyright', 'and', 'guards', 'it', 'ferociously', '.', '(', 'thomas', 'peter/reuters', ')']\n**********\n['the', 'city', 'that', 'was', 'the', 'center', 'of', 'adolf', 'hitler', '’', 's', 'empire', 'is', 'littered', 'with', 'reminders', 'of', 'the', 'nazi', 'past', ',', 'from', 'the', 'bullet', 'holes', 'that', 'pit', 'the', 'fronts', 'of', 'many', 'buildings', 'to', 'the', 'hulking', 'luftwaffe', 'headquarters', 'that', 'now', 'house', 'the', 'finance', 'ministry', '.']\n**********\n['what', 'it', 'doesn', '’', 't', 'have', ',', 'nor', 'has', 'it', 'since', '1945', ',', 'are', 'copies', 'of', 'hitler', '’', 's', 'autobiography', 'and', 'political', 'manifesto', ',', '“', 'mein', 'kampf', ',', '”', 'in', 'its', 'bookstores', '.', 'the', 'latest', 'attempt', 'to', 'publish', 'excerpts', 'fizzled', 'this', 'week', 'after', 'the', 'bavarian', 'government', 'challenged', 'it', 'in', 'court', ',', 'although', 'an', 'expurgated', 'copy', 'appeared', 'at', 'newspaper', 'kiosks', 'around', 'the', 'country', '.']\n**********\n['but', 'in', 'germany', '—', 'where', 'keeping', 'a', 'tight', 'lid', 'on', 'hitler', '’', 's', 'writings', 'has', 'become', 'a', 'rich', 'tradition', 'in', 'itself', '—', 'attitudes', 'toward', 'his', 'book', 'are', 'slowly', 'changing', ',', 'and', 'fewer', 'people', 'are', 'objecting', 'to', 'its', 'becoming', 'more', 'widely', 'available', '.']\n**********\n['no', 'law', 'bans', '“', 'mein', 'kampf', '”', 'in', 'germany', ',', 'but', 'the', 'government', 'of', 'bavaria', ',', 'where', 'hitler', 'officially', 'resided', 'at', 'the', 'time', 'of', 'his', '1945', 'suicide', ',', 'holds', 'the', 'copyright', 'and', 'guards', 'it', 'ferociously', '.', 'german-language', 'copies', 'that', 'were', 'printed', 'before', '1945', 'are', 'legal', ',', 'although', 'they', 'command', 'a', 'premium', 'price', ',', 'and', 'the', 'book', 'is', 'available', 'in', 'translation', 'elsewhere', 'in', 'the', 'world', '.']\n**********\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def get_tokenized_data(data):\n    tokenized_list = []\n    for i in range(1000):\n        text = data[i]['text']\n        sentences_list = split_to_sentences(text)\n        tokenized_list.extend(tokenize_sentences(sentences_list))\n\n    return tokenized_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:42.149925Z","iopub.execute_input":"2025-03-25T05:11:42.150209Z","iopub.status.idle":"2025-03-25T05:11:42.154531Z","shell.execute_reply.started":"2025-03-25T05:11:42.150188Z","shell.execute_reply":"2025-03-25T05:11:42.153618Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"tokenized_data = get_tokenized_data(dataset)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:44.681859Z","iopub.execute_input":"2025-03-25T05:11:44.682236Z","iopub.status.idle":"2025-03-25T05:11:50.868240Z","shell.execute_reply.started":"2025-03-25T05:11:44.682205Z","shell.execute_reply":"2025-03-25T05:11:50.867587Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"tokenized_data=[tokenized_l for tokenized_l in tokenized_data if len(tokenized_l)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:56.439678Z","iopub.execute_input":"2025-03-25T05:11:56.440070Z","iopub.status.idle":"2025-03-25T05:11:56.453149Z","shell.execute_reply.started":"2025-03-25T05:11:56.440041Z","shell.execute_reply":"2025-03-25T05:11:56.452150Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"for i in range(5):\n    print(tokenized_data[i])\n    print('*'*10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:11:58.487556Z","iopub.execute_input":"2025-03-25T05:11:58.487900Z","iopub.status.idle":"2025-03-25T05:11:58.494355Z","shell.execute_reply.started":"2025-03-25T05:11:58.487868Z","shell.execute_reply":"2025-03-25T05:11:58.493664Z"}},"outputs":[{"name":"stdout","text":"['a', 'magazine', 'supplement', 'with', 'an', 'image', 'of', 'adolf', 'hitler', 'and', 'the', 'title', \"'the\", 'unreadable', 'book', \"'\", 'is', 'pictured', 'in', 'berlin', '.', 'no', 'law', 'bans', '“', 'mein', 'kampf', '”', 'in', 'germany', ',', 'but', 'the', 'government', 'of', 'bavaria', ',', 'holds', 'the', 'copyright', 'and', 'guards', 'it', 'ferociously', '.', '(', 'thomas', 'peter/reuters', ')']\n**********\n['the', 'city', 'that', 'was', 'the', 'center', 'of', 'adolf', 'hitler', '’', 's', 'empire', 'is', 'littered', 'with', 'reminders', 'of', 'the', 'nazi', 'past', ',', 'from', 'the', 'bullet', 'holes', 'that', 'pit', 'the', 'fronts', 'of', 'many', 'buildings', 'to', 'the', 'hulking', 'luftwaffe', 'headquarters', 'that', 'now', 'house', 'the', 'finance', 'ministry', '.']\n**********\n['what', 'it', 'doesn', '’', 't', 'have', ',', 'nor', 'has', 'it', 'since', '1945', ',', 'are', 'copies', 'of', 'hitler', '’', 's', 'autobiography', 'and', 'political', 'manifesto', ',', '“', 'mein', 'kampf', ',', '”', 'in', 'its', 'bookstores', '.', 'the', 'latest', 'attempt', 'to', 'publish', 'excerpts', 'fizzled', 'this', 'week', 'after', 'the', 'bavarian', 'government', 'challenged', 'it', 'in', 'court', ',', 'although', 'an', 'expurgated', 'copy', 'appeared', 'at', 'newspaper', 'kiosks', 'around', 'the', 'country', '.']\n**********\n['but', 'in', 'germany', '—', 'where', 'keeping', 'a', 'tight', 'lid', 'on', 'hitler', '’', 's', 'writings', 'has', 'become', 'a', 'rich', 'tradition', 'in', 'itself', '—', 'attitudes', 'toward', 'his', 'book', 'are', 'slowly', 'changing', ',', 'and', 'fewer', 'people', 'are', 'objecting', 'to', 'its', 'becoming', 'more', 'widely', 'available', '.']\n**********\n['no', 'law', 'bans', '“', 'mein', 'kampf', '”', 'in', 'germany', ',', 'but', 'the', 'government', 'of', 'bavaria', ',', 'where', 'hitler', 'officially', 'resided', 'at', 'the', 'time', 'of', 'his', '1945', 'suicide', ',', 'holds', 'the', 'copyright', 'and', 'guards', 'it', 'ferociously', '.', 'german-language', 'copies', 'that', 'were', 'printed', 'before', '1945', 'are', 'legal', ',', 'although', 'they', 'command', 'a', 'premium', 'price', ',', 'and', 'the', 'book', 'is', 'available', 'in', 'translation', 'elsewhere', 'in', 'the', 'world', '.']\n**********\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"# train test split\nrandom.seed(87)\nrandom.shuffle(tokenized_data)\n\ntrain_size = int(len(tokenized_data) * 0.8)\ntrain_data = tokenized_data[0:train_size]\ntest_data = tokenized_data[train_size:]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:04.428819Z","iopub.execute_input":"2025-03-25T05:12:04.429104Z","iopub.status.idle":"2025-03-25T05:12:04.444300Z","shell.execute_reply.started":"2025-03-25T05:12:04.429080Z","shell.execute_reply":"2025-03-25T05:12:04.443292Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"print(\"train data overview\")\nfor i in range(5):\n    print(train_data[i])\n    print('*'*10)\nprint()\nprint(\"test data overview\")\nfor i in range(5):\n    print(test_data[i])\n    print('*'*10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:06.689614Z","iopub.execute_input":"2025-03-25T05:12:06.689946Z","iopub.status.idle":"2025-03-25T05:12:06.698863Z","shell.execute_reply.started":"2025-03-25T05:12:06.689917Z","shell.execute_reply":"2025-03-25T05:12:06.698135Z"}},"outputs":[{"name":"stdout","text":"train data overview\n['antonio', 'senzatela', 'has', 'the', 'highest', 'walk', 'rate', 'of', 'the', 'three', 'pitchers', 'discussed', 'in', 'this', 'piece', ',', 'at', '(', 'a', 'still', 'very', 'good', ')', '5.5', '%', '.', 'he', 'also', 'has', 'easily', 'the', 'lowest', 'strikeout', 'rate', ',', 'a', 'worrisome', '11.5', '%', '.', 'his', 'fip', 'is', 'an', 'ugly', '5.40', ',', 'roughly', 'double', 'dykstra', '’', 's', 'and', 'castro', '’', 's', '.', 'so', 'why', 'should', 'we', 'care', 'about', 'him', '?', 'there', 'are', 'several', 'reasons', '.']\n**********\n['itempassive_unique_bow_008_x1', '-', 'ravens', 'flock', 'to', 'your', 'side', '.']\n**********\n['the', 'display', 'is', 'called', '“', 'puremotion', 'hd', 'plus.', '”', 'it', 'has', 'a', 'fast', 'refresh', 'rate', 'to', 'reduce', 'blur', 'when', 'scrolling', '.', 'the', 'clearblack', 'display', 'is', 'even', 'better', 'now', ',', 'and', 'the', 'color', 'tones', 'adjust', 'to', 'glare', 'from', 'the', 'sunlight', '.', 'the', 'display', 'can', '“', 'even', 'bee', 'seen', 'in', 'the', 'dessert', '.', '”']\n**********\n['7.', 'honduras', ',', 'isles', 'of', 'man', '-', 'governments']\n**********\n['myth', '#', '2', '-', 'what', 'is', 'better', ',', 'li-po', 'or', 'li-ion', '?']\n**********\n\ntest data overview\n['does', 'he', 'do', 'weddings', '?', '``', 'i', 'have', 'not', 'done', 'weddings', '.', \"''\"]\n**********\n['february', '2008']\n**********\n['last', 'week', ',', 'in', 'response', 'to', 'martin', '’', 's', 'questions', 'about', 'tunnel', 'operations', ',', 'metro', 'staff', 'slipped', 'in', 'this', 'quiet', 'bombshell', ':']\n**********\n['kps', 'teacher', '``', 'walk-in', \"''\"]\n**********\n['a', 'recent', 'graduate', 'of', 'the', 'famed', 'ucal', 'golden', 'bears', 'program', ',', 'the', 'big', 'back', 'rower', 'is', 'more', 'rugged', 'than', 'what', 'one', 'might', 'expect', 'from', 'someone', 'out', 'of', 'san', 'francisco', '.', 'after', 'some', 'eye-catching', 'displays', 'in', 'the', 'arc', 'tournament', 'he', '’', 's', 'been', 'thrown', 'in', 'at', 'the', 'deep', 'end', 'in', 'the', 'irb', 'circuit', ',', 'to', 'which', 'he', 'has', 'responded', 'superbly', '.', 'somewhat', 'ungainly', 'but', 'deceivingly', 'quick', ',', 'his', 'trademarks', 'are', 'a', 'piston-like', 'fend', '(', 'as', 'canadian', 'harry', 'jones', 'can', 'attest', 'to', ')', 'and', 'thumping', 'tackles', 'that', 'leave', 'even', 'the', 'spectators', 'wincing', '.']\n**********\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def count_words(tokenized_sentences):\n    word_counts={}\n\n    for sentence_l in tokenized_sentences:\n        for token in sentence_l:\n            if token not in word_counts.keys():\n                word_counts[token]=1\n            else:\n                word_counts[token]+=1\n\n    return word_counts","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:13.923119Z","iopub.execute_input":"2025-03-25T05:12:13.923400Z","iopub.status.idle":"2025-03-25T05:12:13.927636Z","shell.execute_reply.started":"2025-03-25T05:12:13.923377Z","shell.execute_reply":"2025-03-25T05:12:13.926810Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# test your code\ntokenized_sentences = [['sky', 'is', 'blue', '.'],\n                       ['leaves', 'are', 'green', '.'],\n                       ['roses', 'are', 'red', '.']]\ncount_words(tokenized_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:16.826294Z","iopub.execute_input":"2025-03-25T05:12:16.826570Z","iopub.status.idle":"2025-03-25T05:12:16.832392Z","shell.execute_reply.started":"2025-03-25T05:12:16.826548Z","shell.execute_reply":"2025-03-25T05:12:16.831503Z"}},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"{'sky': 1,\n 'is': 1,\n 'blue': 1,\n '.': 3,\n 'leaves': 1,\n 'are': 2,\n 'green': 1,\n 'roses': 1,\n 'red': 1}"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n   \n    closed_vocab = []\n    word_counts = count_words(tokenized_sentences)\n\n    for word, cnt in word_counts.items(): # complete this line\n        \n        if cnt >= count_threshold:\n            closed_vocab.append(word)\n    \n    return closed_vocab","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:19.156035Z","iopub.execute_input":"2025-03-25T05:12:19.156335Z","iopub.status.idle":"2025-03-25T05:12:19.160518Z","shell.execute_reply.started":"2025-03-25T05:12:19.156308Z","shell.execute_reply":"2025-03-25T05:12:19.159566Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"tokenized_sentences = [['sky', 'is', 'blue', '.'],\n                       ['leaves', 'are', 'green', '.'],\n                       ['roses', 'are', 'red', '.']]\ntmp_closed_vocab = get_words_with_nplus_frequency(tokenized_sentences, count_threshold=2)\nprint(f\"Closed vocabulary:\")\nprint(tmp_closed_vocab)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:21.181085Z","iopub.execute_input":"2025-03-25T05:12:21.181393Z","iopub.status.idle":"2025-03-25T05:12:21.186885Z","shell.execute_reply.started":"2025-03-25T05:12:21.181368Z","shell.execute_reply":"2025-03-25T05:12:21.186038Z"}},"outputs":[{"name":"stdout","text":"Closed vocabulary:\n['.', 'are']\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n    vocabulary = set(vocabulary)\n    replaced_tokenized_sentences = []\n    \n    for sentence in tokenized_sentences:\n        replaced_sentence = []\n        \n        for token in sentence: \n            if token in vocabulary: \n                replaced_sentence.append(token)\n            else:\n                replaced_sentence.append(unknown_token)\n        \n        replaced_tokenized_sentences.append(replaced_sentence)\n    return replaced_tokenized_sentences","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:25.412862Z","iopub.execute_input":"2025-03-25T05:12:25.413157Z","iopub.status.idle":"2025-03-25T05:12:25.417841Z","shell.execute_reply.started":"2025-03-25T05:12:25.413134Z","shell.execute_reply":"2025-03-25T05:12:25.416863Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"tokenized_sentences = [[\"dogs\", \"run\"], [\"cats\", \"sleep\"]]\nvocabulary = [\"dogs\", \"sleep\"]\ntmp_replaced_tokenized_sentences = replace_oov_words_by_unk(tokenized_sentences, vocabulary)\nprint(f\"Original sentence:\")\nprint(tokenized_sentences)\nprint(f\"tokenized_sentences with less frequent words converted to '<unk>':\")\nprint(tmp_replaced_tokenized_sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:12:40.750619Z","iopub.execute_input":"2025-03-25T05:12:40.750974Z","iopub.status.idle":"2025-03-25T05:12:40.756614Z","shell.execute_reply.started":"2025-03-25T05:12:40.750945Z","shell.execute_reply":"2025-03-25T05:12:40.755926Z"}},"outputs":[{"name":"stdout","text":"Original sentence:\n[['dogs', 'run'], ['cats', 'sleep']]\ntokenized_sentences with less frequent words converted to '<unk>':\n[['dogs', '<unk>'], ['<unk>', 'sleep']]\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def preprocess_data(train_data, test_data, count_threshold, unknown_token=\"<unk>\", get_words_with_nplus_frequency=get_words_with_nplus_frequency, replace_oov_words_by_unk=replace_oov_words_by_unk):\n    # Get the closed vocabulary using the train data\n    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n    # For the train data, replace less common words with \"<unk>\"\n    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary, unknown_token)\n    \n    # For the test data, replace less common words with \"<unk>\"\n    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary, unknown_token)\n    return train_data_replaced, test_data_replaced, vocabulary","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:14:43.243595Z","iopub.execute_input":"2025-03-25T05:14:43.243963Z","iopub.status.idle":"2025-03-25T05:14:43.247994Z","shell.execute_reply.started":"2025-03-25T05:14:43.243933Z","shell.execute_reply":"2025-03-25T05:14:43.247213Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"minimum_freq = 2\ntrain_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, \n                                                                        test_data, \n                                                                        minimum_freq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:15:14.523791Z","iopub.execute_input":"2025-03-25T05:15:14.524084Z","iopub.status.idle":"2025-03-25T05:15:14.869470Z","shell.execute_reply.started":"2025-03-25T05:15:14.524063Z","shell.execute_reply":"2025-03-25T05:15:14.868809Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"print(\"First preprocessed training sample:\")\nprint(train_data_processed[0])\nprint()\nprint(\"First preprocessed test sample:\")\nprint(test_data_processed[0])\nprint()\nprint(\"First 10 vocabulary:\")\nprint(vocabulary[0:10])\nprint()\nprint(\"Size of vocabulary:\", len(vocabulary))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:15:32.044661Z","iopub.execute_input":"2025-03-25T05:15:32.045030Z","iopub.status.idle":"2025-03-25T05:15:32.052147Z","shell.execute_reply.started":"2025-03-25T05:15:32.045001Z","shell.execute_reply":"2025-03-25T05:15:32.051265Z"}},"outputs":[{"name":"stdout","text":"First preprocessed training sample:\n['antonio', 'senzatela', 'has', 'the', 'highest', 'walk', 'rate', 'of', 'the', 'three', 'pitchers', 'discussed', 'in', 'this', 'piece', ',', 'at', '(', 'a', 'still', 'very', 'good', ')', '5.5', '%', '.', 'he', 'also', 'has', 'easily', 'the', 'lowest', 'strikeout', 'rate', ',', 'a', 'worrisome', '11.5', '%', '.', 'his', 'fip', 'is', 'an', 'ugly', '5.40', ',', 'roughly', 'double', 'dykstra', '’', 's', 'and', 'castro', '’', 's', '.', 'so', 'why', 'should', 'we', 'care', 'about', 'him', '?', 'there', 'are', 'several', 'reasons', '.']\n\nFirst preprocessed test sample:\n['does', 'he', 'do', '<unk>', '?', '``', 'i', 'have', 'not', 'done', '<unk>', '.', \"''\"]\n\nFirst 10 vocabulary:\n['antonio', 'senzatela', 'has', 'the', 'highest', 'walk', 'rate', 'of', 'three', 'pitchers']\n\nSize of vocabulary: 21799\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def count_n_grams(data, n, start_token='<s>', end_token = '<e>'):\n    \n    n_grams = {}\n\n    for sentence in data: \n        sentence = [start_token]*n + sentence + [end_token]\n        \n        sentence = tuple(sentence)\n        \n        for i in range(len(sentence)-n+1): # complete this line\n\n            n_gram = sentence[i:i+n]\n            \n            if n_gram in n_grams.keys(): \n                n_grams[n_gram] += 1\n            else:\n                n_grams[n_gram] = 1\n    return n_grams","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:23:28.275363Z","iopub.execute_input":"2025-03-25T05:23:28.275689Z","iopub.status.idle":"2025-03-25T05:23:28.280639Z","shell.execute_reply.started":"2025-03-25T05:23:28.275664Z","shell.execute_reply":"2025-03-25T05:23:28.279717Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nprint(\"Uni-gram:\")\nprint(count_n_grams(sentences, 1))\nprint(\"Bi-gram:\")\nprint(count_n_grams(sentences, 2))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:23:38.926961Z","iopub.execute_input":"2025-03-25T05:23:38.927256Z","iopub.status.idle":"2025-03-25T05:23:38.933388Z","shell.execute_reply.started":"2025-03-25T05:23:38.927233Z","shell.execute_reply":"2025-03-25T05:23:38.932407Z"}},"outputs":[{"name":"stdout","text":"Uni-gram:\n{('<s>',): 2, ('i',): 1, ('like',): 2, ('a',): 2, ('cat',): 2, ('<e>',): 2, ('this',): 1, ('dog',): 1, ('is',): 1}\nBi-gram:\n{('<s>', '<s>'): 2, ('<s>', 'i'): 1, ('i', 'like'): 1, ('like', 'a'): 2, ('a', 'cat'): 2, ('cat', '<e>'): 2, ('<s>', 'this'): 1, ('this', 'dog'): 1, ('dog', 'is'): 1, ('is', 'like'): 1}\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"def estimate_probability(word, previous_n_gram, \n                         n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n    previous_n_gram = tuple(previous_n_gram)\n    \n    previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n        \n    # apply k-smoothing\n    denominator = previous_n_gram_count + k * vocabulary_size\n\n    # Define n plus 1 gram as the previous n-gram plus the current word as a tuple\n    n_plus1_gram = previous_n_gram + (word,)\n  \n    n_plus1_gram_count = n_plus1_gram_counts[n_plus1_gram] if n_plus1_gram in n_plus1_gram_counts else 0\n    \n    numerator = n_plus1_gram_count + k\n    probability = numerator / denominator\n    \n    return probability","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:27:42.532037Z","iopub.execute_input":"2025-03-25T05:27:42.532364Z","iopub.status.idle":"2025-03-25T05:27:42.536853Z","shell.execute_reply.started":"2025-03-25T05:27:42.532340Z","shell.execute_reply":"2025-03-25T05:27:42.536049Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\ntmp_prob = estimate_probability(\"cat\", [\"a\"], unigram_counts, bigram_counts, len(unique_words), k=1)\n\nprint(f\"The estimated probability of word 'cat' given the previous n-gram 'a' is: {tmp_prob:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:28:24.762588Z","iopub.execute_input":"2025-03-25T05:28:24.762923Z","iopub.status.idle":"2025-03-25T05:28:24.768690Z","shell.execute_reply.started":"2025-03-25T05:28:24.762891Z","shell.execute_reply":"2025-03-25T05:28:24.767455Z"}},"outputs":[{"name":"stdout","text":"The estimated probability of word 'cat' given the previous n-gram 'a' is: 0.3333\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"def estimate_probabilities(previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\",  k=1.0):\n    \n    previous_n_gram = tuple(previous_n_gram)    \n    \n    vocabulary = vocabulary + [end_token, unknown_token]    \n    vocabulary_size = len(vocabulary)    \n    \n    probabilities = {}\n    for word in vocabulary:\n        probability = estimate_probability(word, previous_n_gram, \n                                           n_gram_counts, n_plus1_gram_counts, \n                                           vocabulary_size, k=k)\n                \n        probabilities[word] = probability\n\n    return probabilities","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:29:21.350009Z","iopub.execute_input":"2025-03-25T05:29:21.350340Z","iopub.status.idle":"2025-03-25T05:29:21.355148Z","shell.execute_reply.started":"2025-03-25T05:29:21.350309Z","shell.execute_reply":"2025-03-25T05:29:21.354190Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\nestimate_probabilities([\"a\"], unigram_counts, bigram_counts, unique_words, k=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:29:45.024382Z","iopub.execute_input":"2025-03-25T05:29:45.024702Z","iopub.status.idle":"2025-03-25T05:29:45.031631Z","shell.execute_reply.started":"2025-03-25T05:29:45.024678Z","shell.execute_reply":"2025-03-25T05:29:45.030813Z"}},"outputs":[{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"{'a': 0.09090909090909091,\n 'is': 0.09090909090909091,\n 'like': 0.09090909090909091,\n 'i': 0.09090909090909091,\n 'cat': 0.2727272727272727,\n 'dog': 0.09090909090909091,\n 'this': 0.09090909090909091,\n '<e>': 0.09090909090909091,\n '<unk>': 0.09090909090909091}"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"trigram_counts = count_n_grams(sentences, 3)\nestimate_probabilities([\"<s>\", \"<s>\"], bigram_counts, trigram_counts, unique_words, k=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:30:09.068818Z","iopub.execute_input":"2025-03-25T05:30:09.069112Z","iopub.status.idle":"2025-03-25T05:30:09.074442Z","shell.execute_reply.started":"2025-03-25T05:30:09.069089Z","shell.execute_reply":"2025-03-25T05:30:09.073669Z"}},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"{'a': 0.09090909090909091,\n 'is': 0.09090909090909091,\n 'like': 0.09090909090909091,\n 'i': 0.18181818181818182,\n 'cat': 0.09090909090909091,\n 'dog': 0.09090909090909091,\n 'this': 0.18181818181818182,\n '<e>': 0.09090909090909091,\n '<unk>': 0.09090909090909091}"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"def make_count_matrix(n_plus1_gram_counts, vocabulary):\n    # add <e> <unk> to the vocabulary\n    # <s> is omitted since it should not appear as the next word\n    vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n    \n    # obtain unique n-grams\n    n_grams = []\n    for n_plus1_gram in n_plus1_gram_counts.keys():\n        n_gram = n_plus1_gram[0:-1]        \n        n_grams.append(n_gram)\n    n_grams = list(set(n_grams))\n    \n    # mapping from n-gram to row\n    row_index = {n_gram:i for i, n_gram in enumerate(n_grams)}    \n    # mapping from next word to column\n    col_index = {word:j for j, word in enumerate(vocabulary)}    \n    \n    nrow = len(n_grams)\n    ncol = len(vocabulary)\n    count_matrix = np.zeros((nrow, ncol))\n    for n_plus1_gram, count in n_plus1_gram_counts.items():\n        n_gram = n_plus1_gram[0:-1]\n        word = n_plus1_gram[-1]\n        if word not in vocabulary:\n            continue\n        i = row_index[n_gram]\n        j = col_index[word]\n        count_matrix[i, j] = count\n    \n    count_matrix = pd.DataFrame(count_matrix, index=n_grams, columns=vocabulary)\n    return count_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:31:42.816741Z","iopub.execute_input":"2025-03-25T05:31:42.817113Z","iopub.status.idle":"2025-03-25T05:31:42.823342Z","shell.execute_reply.started":"2025-03-25T05:31:42.817087Z","shell.execute_reply":"2025-03-25T05:31:42.822250Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nbigram_counts = count_n_grams(sentences, 2)\n\nprint('bigram counts')\ndisplay(make_count_matrix(bigram_counts, unique_words))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:31:55.901687Z","iopub.execute_input":"2025-03-25T05:31:55.902034Z","iopub.status.idle":"2025-03-25T05:31:55.937684Z","shell.execute_reply.started":"2025-03-25T05:31:55.902005Z","shell.execute_reply":"2025-03-25T05:31:55.937009Z"}},"outputs":[{"name":"stdout","text":"bigram counts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"           a   is  like    i  cat  dog  this  <e>  <unk>\n(dog,)   0.0  1.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(like,)  2.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(i,)     0.0  0.0   1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(is,)    0.0  0.0   1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(this,)  0.0  0.0   0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(<s>,)   0.0  0.0   0.0  1.0  0.0  0.0   1.0  0.0    0.0\n(cat,)   0.0  0.0   0.0  0.0  0.0  0.0   0.0  2.0    0.0\n(a,)     0.0  0.0   0.0  0.0  2.0  0.0   0.0  0.0    0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>is</th>\n      <th>like</th>\n      <th>i</th>\n      <th>cat</th>\n      <th>dog</th>\n      <th>this</th>\n      <th>&lt;e&gt;</th>\n      <th>&lt;unk&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(dog,)</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(like,)</th>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(i,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(is,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(this,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(&lt;s&gt;,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(cat,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(a,)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"print('\\ntrigram counts')\ntrigram_counts = count_n_grams(sentences, 3)\ndisplay(make_count_matrix(trigram_counts, unique_words))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:32:20.903607Z","iopub.execute_input":"2025-03-25T05:32:20.903931Z","iopub.status.idle":"2025-03-25T05:32:20.923003Z","shell.execute_reply.started":"2025-03-25T05:32:20.903902Z","shell.execute_reply":"2025-03-25T05:32:20.922015Z"}},"outputs":[{"name":"stdout","text":"\ntrigram counts\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"               a   is  like    i  cat  dog  this  <e>  <unk>\n(<s>, <s>)   0.0  0.0   0.0  1.0  0.0  0.0   1.0  0.0    0.0\n(dog, is)    0.0  0.0   1.0  0.0  0.0  0.0   0.0  0.0    0.0\n(<s>, this)  0.0  0.0   0.0  0.0  0.0  1.0   0.0  0.0    0.0\n(this, dog)  0.0  1.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(i, like)    1.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(a, cat)     0.0  0.0   0.0  0.0  0.0  0.0   0.0  2.0    0.0\n(is, like)   1.0  0.0   0.0  0.0  0.0  0.0   0.0  0.0    0.0\n(like, a)    0.0  0.0   0.0  0.0  2.0  0.0   0.0  0.0    0.0\n(<s>, i)     0.0  0.0   1.0  0.0  0.0  0.0   0.0  0.0    0.0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>is</th>\n      <th>like</th>\n      <th>i</th>\n      <th>cat</th>\n      <th>dog</th>\n      <th>this</th>\n      <th>&lt;e&gt;</th>\n      <th>&lt;unk&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(dog, is)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(&lt;s&gt;, this)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(this, dog)</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(i, like)</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(a, cat)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(is, like)</th>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(like, a)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>(&lt;s&gt;, i)</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"def make_probability_matrix(n_plus1_gram_counts, vocabulary, k):\n    count_matrix = make_count_matrix(n_plus1_gram_counts, unique_words)\n    count_matrix += k\n    prob_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n    return prob_matrix","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:33:14.568072Z","iopub.execute_input":"2025-03-25T05:33:14.568397Z","iopub.status.idle":"2025-03-25T05:33:14.572457Z","shell.execute_reply.started":"2025-03-25T05:33:14.568367Z","shell.execute_reply":"2025-03-25T05:33:14.571668Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\nbigram_counts = count_n_grams(sentences, 2)\nprint(\"bigram probabilities\")\ndisplay(make_probability_matrix(bigram_counts, unique_words, k=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:33:44.875256Z","iopub.execute_input":"2025-03-25T05:33:44.875563Z","iopub.status.idle":"2025-03-25T05:33:44.895508Z","shell.execute_reply.started":"2025-03-25T05:33:44.875541Z","shell.execute_reply":"2025-03-25T05:33:44.894761Z"}},"outputs":[{"name":"stdout","text":"bigram probabilities\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                a        is      like         i       cat       dog      this  \\\n(dog,)   0.100000  0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n(like,)  0.272727  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n(i,)     0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n(is,)    0.100000  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n(this,)  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000  0.100000   \n(<s>,)   0.090909  0.090909  0.090909  0.181818  0.090909  0.090909  0.181818   \n(cat,)   0.090909  0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n(a,)     0.090909  0.090909  0.090909  0.090909  0.272727  0.090909  0.090909   \n\n              <e>     <unk>  \n(dog,)   0.100000  0.100000  \n(like,)  0.090909  0.090909  \n(i,)     0.100000  0.100000  \n(is,)    0.100000  0.100000  \n(this,)  0.100000  0.100000  \n(<s>,)   0.090909  0.090909  \n(cat,)   0.272727  0.090909  \n(a,)     0.090909  0.090909  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>is</th>\n      <th>like</th>\n      <th>i</th>\n      <th>cat</th>\n      <th>dog</th>\n      <th>this</th>\n      <th>&lt;e&gt;</th>\n      <th>&lt;unk&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(dog,)</th>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(like,)</th>\n      <td>0.272727</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>(i,)</th>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(is,)</th>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(this,)</th>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(&lt;s&gt;,)</th>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.181818</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.181818</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>(cat,)</th>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.272727</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>(a,)</th>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.272727</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":40},{"cell_type":"code","source":"print(\"trigram probabilities\")\ntrigram_counts = count_n_grams(sentences, 3)\ndisplay(make_probability_matrix(trigram_counts, unique_words, k=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:33:56.738696Z","iopub.execute_input":"2025-03-25T05:33:56.739043Z","iopub.status.idle":"2025-03-25T05:33:56.756172Z","shell.execute_reply.started":"2025-03-25T05:33:56.739014Z","shell.execute_reply":"2025-03-25T05:33:56.755143Z"}},"outputs":[{"name":"stdout","text":"trigram probabilities\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"                    a        is      like         i       cat       dog  \\\n(<s>, <s>)   0.090909  0.090909  0.090909  0.181818  0.090909  0.090909   \n(dog, is)    0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n(<s>, this)  0.100000  0.100000  0.100000  0.100000  0.100000  0.200000   \n(this, dog)  0.100000  0.200000  0.100000  0.100000  0.100000  0.100000   \n(i, like)    0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n(a, cat)     0.090909  0.090909  0.090909  0.090909  0.090909  0.090909   \n(is, like)   0.200000  0.100000  0.100000  0.100000  0.100000  0.100000   \n(like, a)    0.090909  0.090909  0.090909  0.090909  0.272727  0.090909   \n(<s>, i)     0.100000  0.100000  0.200000  0.100000  0.100000  0.100000   \n\n                 this       <e>     <unk>  \n(<s>, <s>)   0.181818  0.090909  0.090909  \n(dog, is)    0.100000  0.100000  0.100000  \n(<s>, this)  0.100000  0.100000  0.100000  \n(this, dog)  0.100000  0.100000  0.100000  \n(i, like)    0.100000  0.100000  0.100000  \n(a, cat)     0.090909  0.272727  0.090909  \n(is, like)   0.100000  0.100000  0.100000  \n(like, a)    0.090909  0.090909  0.090909  \n(<s>, i)     0.100000  0.100000  0.100000  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a</th>\n      <th>is</th>\n      <th>like</th>\n      <th>i</th>\n      <th>cat</th>\n      <th>dog</th>\n      <th>this</th>\n      <th>&lt;e&gt;</th>\n      <th>&lt;unk&gt;</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>(&lt;s&gt;, &lt;s&gt;)</th>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.181818</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.181818</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>(dog, is)</th>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(&lt;s&gt;, this)</th>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(this, dog)</th>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(i, like)</th>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(a, cat)</th>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.272727</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>(is, like)</th>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>(like, a)</th>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.272727</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n      <td>0.090909</td>\n    </tr>\n    <tr>\n      <th>(&lt;s&gt;, i)</th>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.200000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n      <td>0.100000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":41},{"cell_type":"code","source":"def calculate_perplexity(sentence, n_gram_counts, n_plus1_gram_counts, vocabulary_size, start_token='<s>', end_token = '<e>', k=1.0):\n    n = len(list(n_gram_counts.keys())[0]) \n    \n    # prepend <s> and append <e>\n    sentence = [start_token] * n + sentence + [end_token]\n    \n    sentence = tuple(sentence)\n    N = len(sentence)\n    product_pi = 1.0\n    \n    for t in range(n, N):\n\n        # get the n-gram preceding the word at position t\n        n_gram = sentence[t-n:t]\n        \n        # get the word at position t\n        word = sentence[t]\n        probability = estimate_probability(word, n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k)\n    \n        product_pi *= 1/probability\n        perplexity = (product_pi)**(1/N)\n    return perplexity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:38:08.479333Z","iopub.execute_input":"2025-03-25T05:38:08.479690Z","iopub.status.idle":"2025-03-25T05:38:08.485371Z","shell.execute_reply.started":"2025-03-25T05:38:08.479660Z","shell.execute_reply":"2025-03-25T05:38:08.484283Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n                 ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\n\nperplexity_train = calculate_perplexity(sentences[0],\n                                         unigram_counts, bigram_counts,\n                                         len(unique_words), k=1.0)\nprint(f\"Perplexity for first train sample: {perplexity_train:.4f}\")\n\ntest_sentence = ['i', 'like', 'a', 'dog']\nperplexity_test = calculate_perplexity(test_sentence,\n                                       unigram_counts, bigram_counts,\n                                       len(unique_words), k=1.0)\nprint(f\"Perplexity for test sample: {perplexity_test:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:38:41.754325Z","iopub.execute_input":"2025-03-25T05:38:41.754613Z","iopub.status.idle":"2025-03-25T05:38:41.761212Z","shell.execute_reply.started":"2025-03-25T05:38:41.754590Z","shell.execute_reply":"2025-03-25T05:38:41.760132Z"}},"outputs":[{"name":"stdout","text":"Perplexity for first train sample: 2.8040\nPerplexity for test sample: 3.9654\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"def suggest_a_word(previous_tokens, n_gram_counts, n_plus1_gram_counts, vocabulary, end_token='<e>', unknown_token=\"<unk>\", k=1.0, start_with=None):\n\n    n = len(list(n_gram_counts.keys())[0]) \n    previous_n_gram = previous_tokens[-n:]\n\n    probabilities = estimate_probabilities(previous_n_gram,\n                                           n_gram_counts, n_plus1_gram_counts,\n                                           vocabulary, k=k)\n    suggestion = None\n    \n    max_prob = 0\n    \n    for word, prob in probabilities.items(): # complete this line\n        if start_with:\n            if not word.startswith(start_with): \n                continue\n        \n        if prob > max_prob: \n            suggestion = word\n            max_prob = prob\n\n \n    return suggestion, max_prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:46:37.717618Z","iopub.execute_input":"2025-03-25T05:46:37.717993Z","iopub.status.idle":"2025-03-25T05:46:37.723442Z","shell.execute_reply.started":"2025-03-25T05:46:37.717963Z","shell.execute_reply":"2025-03-25T05:46:37.722343Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\n\nprevious_tokens = [\"i\", \"like\"]\ntmp_suggest1 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0)\nprint(f\"The previous words are 'i like',\\n\\tand the suggested word is `{tmp_suggest1[0]}` with a probability of {tmp_suggest1[1]:.4f}\")\n\nprint()\n# test your code when setting the starts_with\ntmp_starts_with = 'c'\ntmp_suggest2 = suggest_a_word(previous_tokens, unigram_counts, bigram_counts, unique_words, k=1.0, start_with=tmp_starts_with)\nprint(f\"The previous words are 'i like', the suggestion must start with `{tmp_starts_with}`\\n\\tand the suggested word is `{tmp_suggest2[0]}` with a probability of {tmp_suggest2[1]:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:47:03.850932Z","iopub.execute_input":"2025-03-25T05:47:03.851242Z","iopub.status.idle":"2025-03-25T05:47:03.857629Z","shell.execute_reply.started":"2025-03-25T05:47:03.851217Z","shell.execute_reply":"2025-03-25T05:47:03.856783Z"}},"outputs":[{"name":"stdout","text":"The previous words are 'i like',\n\tand the suggested word is `a` with a probability of 0.2727\n\nThe previous words are 'i like', the suggestion must start with `c`\n\tand the suggested word is `cat` with a probability of 0.0909\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n    model_counts = len(n_gram_counts_list)\n    suggestions = []\n    for i in range(model_counts-1):\n        n_gram_counts = n_gram_counts_list[i]\n        n_plus1_gram_counts = n_gram_counts_list[i+1]\n        \n        suggestion = suggest_a_word(previous_tokens, n_gram_counts,\n                                    n_plus1_gram_counts, vocabulary,\n                                    k=k, start_with=start_with)\n        suggestions.append(suggestion)\n    return suggestions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:48:30.871262Z","iopub.execute_input":"2025-03-25T05:48:30.871617Z","iopub.status.idle":"2025-03-25T05:48:30.876131Z","shell.execute_reply.started":"2025-03-25T05:48:30.871586Z","shell.execute_reply":"2025-03-25T05:48:30.875299Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"sentences = [['i', 'like', 'a', 'cat'],\n             ['this', 'dog', 'is', 'like', 'a', 'cat']]\nunique_words = list(set(sentences[0] + sentences[1]))\n\nunigram_counts = count_n_grams(sentences, 1)\nbigram_counts = count_n_grams(sentences, 2)\ntrigram_counts = count_n_grams(sentences, 3)\nquadgram_counts = count_n_grams(sentences, 4)\nqintgram_counts = count_n_grams(sentences, 5)\n\nn_gram_counts_list = [unigram_counts, bigram_counts, trigram_counts, quadgram_counts, qintgram_counts]\nprevious_tokens = [\"i\", \"like\"]\ntmp_suggest3 = get_suggestions(previous_tokens, n_gram_counts_list, unique_words, k=1.0)\n\nprint(f\"The previous words are 'i like', the suggestions are:\")\ndisplay(tmp_suggest3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:48:57.373976Z","iopub.execute_input":"2025-03-25T05:48:57.374309Z","iopub.status.idle":"2025-03-25T05:48:57.381867Z","shell.execute_reply.started":"2025-03-25T05:48:57.374278Z","shell.execute_reply":"2025-03-25T05:48:57.381095Z"}},"outputs":[{"name":"stdout","text":"The previous words are 'i like', the suggestions are:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[('a', 0.2727272727272727),\n ('a', 0.2),\n ('a', 0.1111111111111111),\n ('a', 0.1111111111111111)]"},"metadata":{}}],"execution_count":47},{"cell_type":"code","source":"n_gram_counts_list = []\nfor n in range(1, 6):\n    print(\"Computing n-gram counts with n =\", n, \"...\")\n    n_model_counts = count_n_grams(train_data_processed, n)\n    n_gram_counts_list.append(n_model_counts)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:50:39.801089Z","iopub.execute_input":"2025-03-25T05:50:39.801432Z","iopub.status.idle":"2025-03-25T05:50:41.707917Z","shell.execute_reply.started":"2025-03-25T05:50:39.801403Z","shell.execute_reply":"2025-03-25T05:50:41.706996Z"}},"outputs":[{"name":"stdout","text":"Computing n-gram counts with n = 1 ...\nComputing n-gram counts with n = 2 ...\nComputing n-gram counts with n = 3 ...\nComputing n-gram counts with n = 4 ...\nComputing n-gram counts with n = 5 ...\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"previous_tokens = [\"i\", \"am\", \"to\"]\ntmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:51:03.591096Z","iopub.execute_input":"2025-03-25T05:51:03.591409Z","iopub.status.idle":"2025-03-25T05:51:03.710616Z","shell.execute_reply.started":"2025-03-25T05:51:03.591382Z","shell.execute_reply":"2025-03-25T05:51:03.709662Z"}},"outputs":[{"name":"stdout","text":"The previous words are ['i', 'am', 'to'], the suggestions are:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[('the', 0.04395207106703853),\n ('deliver', 9.173470323823502e-05),\n ('deliver', 9.173470323823502e-05),\n ('antonio', 4.5869455529562866e-05)]"},"metadata":{}}],"execution_count":49},{"cell_type":"code","source":"previous_tokens = [\"hey\", \"how\", \"are\", \"you\"]\ntmp_suggest8 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=\"d\")\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest8)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:53:16.928595Z","iopub.execute_input":"2025-03-25T05:53:16.928968Z","iopub.status.idle":"2025-03-25T05:53:17.052846Z","shell.execute_reply.started":"2025-03-25T05:53:16.928936Z","shell.execute_reply":"2025-03-25T05:53:17.052129Z"}},"outputs":[{"name":"stdout","text":"The previous words are ['hey', 'how', 'are', 'you'], the suggestions are:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[('don', 0.002117120715826476),\n ('doing', 0.00018322568824149146),\n ('discussed', 4.586104104563174e-05),\n ('discussed', 4.5869455529562866e-05)]"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"previous_tokens = [\"i\", \"am\", \"to\"]\ntmp_suggest4 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest4)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:53:44.727282Z","iopub.execute_input":"2025-03-25T05:53:44.727594Z","iopub.status.idle":"2025-03-25T05:53:44.853256Z","shell.execute_reply.started":"2025-03-25T05:53:44.727569Z","shell.execute_reply":"2025-03-25T05:53:44.852382Z"}},"outputs":[{"name":"stdout","text":"The previous words are ['i', 'am', 'to'], the suggestions are:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[('the', 0.04395207106703853),\n ('deliver', 9.173470323823502e-05),\n ('deliver', 9.173470323823502e-05),\n ('antonio', 4.5869455529562866e-05)]"},"metadata":{}}],"execution_count":51},{"cell_type":"code","source":"previous_tokens = [\"i\", \"want\", \"to\", \"go\"]\ntmp_suggest5 = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n\nprint(f\"The previous words are {previous_tokens}, the suggestions are:\")\ndisplay(tmp_suggest5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:54:10.787176Z","iopub.execute_input":"2025-03-25T05:54:10.787491Z","iopub.status.idle":"2025-03-25T05:54:10.908879Z","shell.execute_reply.started":"2025-03-25T05:54:10.787463Z","shell.execute_reply":"2025-03-25T05:54:10.907998Z"}},"outputs":[{"name":"stdout","text":"The previous words are ['i', 'want', 'to', 'go'], the suggestions are:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[('to', 0.003147057501236344),\n ('to', 0.0010924484500887614),\n ('.', 0.00018339370042639035),\n ('out', 9.173470323823502e-05)]"},"metadata":{}}],"execution_count":52},{"cell_type":"code","source":"while(True):\n    y=input(\"predict a new word?:(1/0) \")\n    if y=='0':\n        break\n    prev_tokens=[]\n    sentence = str(input(\"enter sentence: \"))\n    prev_tokens=[word for word in sentence.split(\" \")]\n    sugg=get_suggestions(prev_tokens,n_gram_counts_list,vocabulary,k=1.0)\n    print(\"suggestions are :\")\n    print()\n    display()\n            ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T06:03:15.042246Z","iopub.execute_input":"2025-03-25T06:03:15.042519Z","iopub.status.idle":"2025-03-25T06:03:16.963581Z","shell.execute_reply.started":"2025-03-25T06:03:15.042496Z","shell.execute_reply":"2025-03-25T06:03:16.962631Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"predict a new word?:(1/0)  0\n"}],"execution_count":54},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}